# **7 贝叶斯分类器**

贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。在开始介绍贝叶斯决策论之前，我们首先来回顾下概率论委员会常委--贝叶斯公式。

![1.png](https://i.loli.net/2018/10/18/5bc83fd7a2575.png)

## **7.1 贝叶斯决策论**

若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“**条件风险**”（conditional risk）。

![2.png](https://i.loli.net/2018/10/18/5bc83fd15db94.png)

我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了**贝叶斯判定准则**（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。

![3.png](https://i.loli.net/2018/10/18/5bc83fd308600.png)

若损失函数λ取0-1损失，则有：

![4.png](https://i.loli.net/2018/10/18/5bc83fd37c502.png)

即对于每个样本x，选择其后验概率P（c|x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c|x）。一般这里有两种策略来对后验概率进行估计：

 + 判别式模型：直接对 P（c|x）进行建模求解。例我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。
 + 生成式模型：通过先对联合分布P（x,c）建模，从而进一步求解 P（c|x）。

贝叶斯分类器就属于生成式模型，基于贝叶斯公式对后验概率P（c|x） 进行变换：

![5.png](https://i.loli.net/2018/10/18/5bc83fd501ad3.png)

对于给定的样本x，P（x）与类标无关，P（c）称为类先验概率，p（x|c）称为类条件概率。这时估计后验概率P（c|x）就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率：

 + 先验概率：根据以往经验和分析得到的概率。
 + 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。

回归正题，对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x|c），它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用极大似然法进行估计。

## **7.2 极大似然法**

极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x|c）中，假设p（x|c）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。

![6.png](https://i.loli.net/2018/10/18/5bc83fd70fb73.png)

所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：

 + 1.写出似然函数；
 + 2.对似然函数取对数，并整理；
 + 3.求导数，令偏导数为0，得到似然方程组；
 + 4.解似然方程组，得到所有参数即为所求。

例如：假设样本属性都是连续值，p（x|c）服从一个多维高斯分布，则通过MLE计算出的参数刚好分别为：

![7.png](https://i.loli.net/2018/10/18/5bc83fd705729.png)

上述结果看起来十分合乎实际，但是采用最大似然法估计参数的效果很大程度上依赖于作出的假设是否合理，是否符合潜在的真实数据分布。这就需要大量的经验知识，搞统计越来越值钱也是这个道理，大牛们掐指一算比我们搬砖几天更有效果。

## **7.3 朴素贝叶斯分类器**

不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。为了避免这个问题，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x|c）可以改写为：

![8.png](https://i.loli.net/2018/10/18/5bc83fd55e102.png)

这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。

![9.png](https://i.loli.net/2018/10/18/5bc83fd6678cd.png)

相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：

![10.png](https://i.loli.net/2018/10/18/5bc83fe54aaed.png)

当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。

## **7.4 半朴素贝叶斯分类器**

对属性条件独立性假设进行一定程度放松，适当考虑一部分属性间的相互依赖信息，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。

## **7.5 贝叶斯网**

贝叶斯网借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表来描述属性的联合概率分布。

## **7.6 EM算法**

EM（Expectation-Maximization）算法是一种常用的估计参数隐变量的利器，也称为“期望最大算法”，是数据挖掘的十大经典算法之一。EM算法主要应用于训练集样本不完整即存在隐变量时的情形（例如某个属性值未知），通过其独特的“两步走”策略能较好地估计出隐变量的值。

## **7.6.1 EM算法思想**

EM是一种迭代式的方法，它的基本思想就是：若样本服从的分布参数θ已知，则可以根据已观测到的训练样本推断出隐变量Z的期望值（E步），若Z的值已知则运用最大似然法估计出新的θ值（M步）。重复这个过程直到Z和θ值不再发生变化。

简单来讲：假设我们想估计A和B这两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。

![1.png](https://i.loli.net/2018/10/18/5bc843bf53eb2.png)

现在再来回想聚类的代表算法K-Means：【首先随机选择类中心=>将样本点划分到类簇中=>重新计算类中心=>不断迭代直至收敛】，不难发现这个过程和EM迭代的方法极其相似，事实上，若将样本的类别看做为“隐变量”（latent variable）Z，类中心看作样本的分布参数θ，K-Means就是通过EM算法来进行迭代的，与我们这里不同的是，K-Means的目标是最小化样本点到其对应类中心的距离和，上述为极大化似然函数。


## **7.6.2 EM算法数学推导**

在上篇极大似然法中，当样本属性值都已知时，我们很容易通过极大化对数似然，接着对每个参数求偏导计算出参数的值。但当存在隐变量时，就无法直接求解，此时我们通常最大化已观察数据的对数“边际似然”（marginal likelihood）。

![2.png](https://i.loli.net/2018/10/18/5bc843bfd84d2.png)

这时候，通过边缘似然将隐变量Z引入进来，对于参数估计，现在与最大似然不同的只是似然函数式中多了一个未知的变量Z，也就是说我们的目标是找到适合的θ和Z让L(θ)最大，这样我们也可以分别对未知的θ和Z求偏导，再令其等于0。

然而观察上式可以发现，和的对数（ln(x1+x2+x3)）求导十分复杂，那能否通过变换上式得到一种求导简单的新表达式呢？这时候 Jensen不等式就派上用场了，先回顾一下高等数学凸函数的内容：

**Jensen's inequality**：过一个凸函数上任意两点所作割线一定在这两点间的函数图象的上方。理解起来也十分简单，对于凸函数f(x)''>0，即曲线的变化率是越来越大单调递增的，所以函数越到后面增长越厉害，这样在一个区间下，函数的均值就会大一些了。

![3.png](https://i.loli.net/2018/10/18/5bc843c064c72.png)

因为ln(*)函数为凹函数，故可以将上式“和的对数”变为“对数的和”，这样就很容易求导了。

![4.png](https://i.loli.net/2018/10/18/5bc843c3490ad.png)

接着求解Qi和θ：首先固定θ（初始值），通过求解Qi使得J（θ，Q）在θ处与L（θ）相等，即求出L（θ）的下界；然后再固定Qi，调整θ，最大化下界J（θ，Q）。不断重复两个步骤直到稳定。通过jensen不等式的性质，Qi的计算公式实际上就是后验概率：

![5.png](https://i.loli.net/2018/10/18/5bc843c21276c.png)

通过数学公式的推导，简单来理解这一过程：固定θ计算Q的过程就是在建立L（θ）的下界，即通过jenson不等式得到的下界（E步）；固定Q计算θ则是使得下界极大化（M步），从而不断推高边缘似然L（θ）。从而循序渐进地计算出L（θ）取得极大值时隐变量Z的估计值。

EM算法也可以看作一种“坐标下降法”，首先固定一个值，对另外一个值求极值，不断重复直到收敛。这时候也许大家就有疑问，问什么不直接这两个家伙求偏导用梯度下降呢？这时候就是坐标下降的优势，有些特殊的函数，例如曲线函数z=y^2+x^2+x^2y+xy+...，无法直接求导，这时如果先固定其中的一个变量，再对另一个变量求极值，则变得可行。

![6.png](https://i.loli.net/2018/10/18/5bc843c34e7ff.png)

## **7.6.3 EM算法流程**

看完数学推导，算法的流程也就十分简单了，这里有两个版本，版本一来自西瓜书，周天使的介绍十分简洁；版本二来自于大牛的博客。结合着数学推导，自认为版本二更具有逻辑性，两者唯一的区别就在于版本二多出了红框的部分，这里我也没得到答案，欢迎骚扰讨论~

**版本一：**

![7.png](https://i.loli.net/2018/10/18/5bc843c0e19db.png)

**版本二：**

![8.png](https://i.loli.net/2018/10/18/5bc843c34775b.png)
